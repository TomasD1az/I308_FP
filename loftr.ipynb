{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00c7aa8",
   "metadata": {},
   "source": [
    "# SI tenemos tiempo, usamos este metodo para sacar los features de la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65220c",
   "metadata": {},
   "source": [
    "### Extracción y matching de features con LoFTR (estado del arte)\n",
    "A continuación, se utiliza LoFTR (a través de la librería kornia) para detectar y matchear features entre las imágenes rectificadas. Es necesario instalar kornia y torch si no están instalados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094c4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar kornia y torch si es necesario (solo la primera vez)\n",
    "# !pip install kornia torch --quiet\n",
    "\n",
    "import torch\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "def loftr_match(img1, img2):\n",
    "    # Convertir a tensores y normalizar\n",
    "    t1 = K.image_to_tensor(img1, False).float() / 255.0  # (1,1,H,W)\n",
    "    t2 = K.image_to_tensor(img2, False).float() / 255.0\n",
    "    if torch.cuda.is_available():\n",
    "        t1 = t1.cuda()\n",
    "        t2 = t2.cuda()\n",
    "    matcher = KF.LoFTR(pretrained=\"outdoor\")\n",
    "    if torch.cuda.is_available():\n",
    "        matcher = matcher.cuda()\n",
    "    input_dict = {\"image0\": t1, \"image1\": t2}\n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "    mkpts0 = correspondences[\"keypoints0\"].cpu().numpy()\n",
    "    mkpts1 = correspondences[\"keypoints1\"].cpu().numpy()\n",
    "    return mkpts0, mkpts1\n",
    "\n",
    "# Matching con LoFTR para el primer par de imágenes\n",
    "loftr_pts1, loftr_pts2 = loftr_match(left_img, right_img)\n",
    "print(f\"LoFTR encontró {loftr_pts1.shape[0]} matches.\")\n",
    "\n",
    "# Visualización de matches\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_loftr_matches(img1, img2, pts1, pts2, n=100):\n",
    "    img1_color = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "    img2_color = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "    h1, w1 = img1.shape\n",
    "    h2, w2 = img2.shape\n",
    "    vis = np.hstack([img1_color, img2_color])\n",
    "    n = min(n, pts1.shape[0])\n",
    "    idx = np.random.choice(pts1.shape[0], n, replace=False)\n",
    "    for i in idx:\n",
    "        pt1 = tuple(np.round(pts1[i]).astype(int))\n",
    "        pt2 = tuple(np.round(pts2[i]).astype(int) + np.array([w1, 0]))\n",
    "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "        cv2.circle(vis, pt1, 3, color, -1)\n",
    "        cv2.circle(vis, pt2, 3, color, -1)\n",
    "        cv2.line(vis, pt1, pt2, color, 1)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(vis[..., ::-1])\n",
    "    plt.axis('off')\n",
    "    plt.title('LoFTR matches (frame 1)')\n",
    "    plt.show()\n",
    "\n",
    "draw_loftr_matches(left_img, right_img, loftr_pts1, loftr_pts2, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb865e4e",
   "metadata": {},
   "source": [
    "### Extracción y matching de features con LoFTR (frame 2)\n",
    "Repetimos el proceso de matching con LoFTR para el segundo par de imágenes rectificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faab850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching con LoFTR para el segundo par de imágenes\n",
    "loftr_pts1_2, loftr_pts2_2 = loftr_match(left_img_2, right_img_2)\n",
    "print(f\"LoFTR encontró {loftr_pts1_2.shape[0]} matches en el frame 2.\")\n",
    "\n",
    "draw_loftr_matches(left_img_2, right_img_2, loftr_pts1_2, loftr_pts2_2, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415b37a",
   "metadata": {},
   "source": [
    "### Asociación de puntos entre frames usando LoFTR (deep learning)\n",
    "Usamos LoFTR para asociar directamente la imagen izquierda del frame 1 con la del frame 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16eeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos LoFTR para asociar directamente la imagen izquierda del frame 1 con la del frame 2\n",
    "loftr_pts1_temporal, loftr_pts2_temporal = loftr_match(left_img, left_img_2)\n",
    "print(f\"LoFTR encontró {loftr_pts1_temporal.shape[0]} matches entre frame 1 y frame 2.\")\n",
    "\n",
    "draw_loftr_matches(left_img, left_img_2, loftr_pts1_temporal, loftr_pts2_temporal, n=100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
